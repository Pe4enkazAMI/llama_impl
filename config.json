{
    "name": "default_config",
    "n_gpu": 1,
    "arch": {
      "type": "LLAMA",
      "args": {
        "embed_dim": 512,
        "num_heads": 8,
        "num_layers": 8
      }
    },
    "data": {
      "train": {
        "batch_size": 256,
        "num_workers": 5,
        "datasets": [
          {
            "type": "LLaMaDataset",
            "args": {
              "data_dir": "/home/jupyter/work/resources/for_dl/train_dataset.pt",
              "tokenizer_model_path": "/Users/bayesian_monster/llama/llm/tiny_stories_5k.model"
            }
          }
        ]
      },
      "test": {
        "batch_size": 256,
        "num_workers": 5,
        "datasets": [
          {
            "type": "LLaMaDataset",
            "args": {
              "data_dir": "/home/jupyter/work/resources/for_dl/val_dataset.pt",
              "tokenizer_model_path": "/Users/bayesian_monster/llama/llm/tiny_stories_5k.model"
            }
          }
        ]
      }
    },
    "optimizer": {
      "type": "AdamW",
      "args": {
        "lr": 1e-3,
        "weight_decay": 1e-5
      }
    },
    "lr_scheduler": {
      "type": "OneCycleLR",
      "args": {
        "steps_per_epoch": 1000,
        "epochs": 100,
        "anneal_strategy": "cos",
        "max_lr": 1e-3,
        "pct_start": 0.2
      }
    },
    "loss": {
      "type": "LLaMaLoss",
      "args": {
        "pad_id": 0
      }
    },
    "metrics": [
    ],
    "trainer": {
      "epochs": 100,
      "save_dir": "saved/",
      "save_period": 5,
      "verbosity": 2,
      "monitor": "min val_loss",
      "early_stop": 100,
      "visualize": "wandb",
      "wandb_project": "lm_project",
      "wandb_run_name": "LLaMa_GO",
      "len_epoch": 1000,
      "grad_norm_clip": 100,
      "grad_accum_iters": 2
    }
  }